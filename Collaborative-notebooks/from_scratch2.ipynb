{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "df_main=pd.read_csv('data/normalized_filtered_user_listening.csv')\n",
    "# df=pd.read_csv('data/user_listening.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_main[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=df.pivot(index='track_id',columns='user_id',values='normalized_playcount')\n",
    "# matrix=df.pivot(index='track_id',columns='user_id',values='playcount')\n",
    "arr=np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique user count : 1877\n",
      "Unique track count : 5077\n"
     ]
    }
   ],
   "source": [
    "print('Unique user count :' , len(df['user_id'].unique()))\n",
    "print('Unique track count :' , len(df['track_id'].unique()))\n",
    "\n",
    "#filled elements\n",
    "known_values=list(zip(np.where(~np.isnan(arr))[0],\n",
    "        np.where(~np.isnan(arr))[1]))\n",
    "\n",
    "#randomly select 30% as testing set\n",
    "lucky_draw=set(np.random.choice(range(len(known_values)),\n",
    "                 size=int(len(known_values)*0.3),\n",
    "                 p=[1/len(known_values)]*len(known_values)))\n",
    "\n",
    "unlucky_draw=[i for i in range(len(known_values)) if i not in lucky_draw]\n",
    "testing_idx=(np.where(~np.isnan(arr))[0][list(lucky_draw)],\n",
    "np.where(~np.isnan(arr))[1][list(lucky_draw)])\n",
    "training_idx=(np.where(~np.isnan(arr))[0][list(unlucky_draw)],\n",
    "np.where(~np.isnan(arr))[1][list(unlucky_draw)])\n",
    "\n",
    "#train test split\n",
    "mask=np.ones(arr.shape)\n",
    "mask[testing_idx]=np.nan\n",
    "arr_train=np.multiply(arr,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koren Neighborhood Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use numba to dramatically boost the speed of linear algebra\n",
    "@numba.njit\n",
    "def koren_neighbor_epoch(arr,miu,b_u,b_i,\n",
    "                         similarity_matrix,\n",
    "                         w_ij,alpha,lambda_,top_k):\n",
    "    \n",
    "    #initialize\n",
    "    error=0\n",
    "    \n",
    "    #only iterate known ratings\n",
    "    for i in range(arr.shape[0]):\n",
    "        for u in range(arr.shape[1]):\n",
    "            r_ui=arr[i,u]\n",
    "            \n",
    "            #another way to identify nan\n",
    "            #r_ui!=r_ui\n",
    "            if np.isnan(r_ui):\n",
    "                continue\n",
    "                \n",
    "            #find top k neighbor based upon similarity matrix\n",
    "            rated_items=np.where(~np.isnan(arr[:,u]))[0]\n",
    "            similarities=similarity_matrix[i][rated_items]\n",
    "            top_k_neighbors=np.argsort(similarities)[-top_k:]\n",
    "            N_k_i_u=np.array([\n",
    "                    rated_items[neighbor] for neighbor in top_k_neighbors])\n",
    "            \n",
    "            #compute error\n",
    "            if len(N_k_i_u)!=0:\n",
    "                deviation=arr[:,u][N_k_i_u]-miu-b_u[u]-b_i[N_k_i_u]\n",
    "                weighted_sum=(w_ij[i][N_k_i_u]).T@deviation\n",
    "                epsilon_ui=(r_ui-miu-b_u[u]-b_i[i]-weighted_sum).item()\n",
    "                error+=epsilon_ui**2\n",
    "            else:\n",
    "                epsilon_ui=(r_ui-miu-b_u[u]-b_i[i]).item()\n",
    "                error+=epsilon_ui**2\n",
    "\n",
    "            #update baseline\n",
    "            b_u[u]+=alpha*(epsilon_ui-lambda_*b_u[u])\n",
    "            b_i[i]+=alpha*(epsilon_ui-lambda_*b_i[i])\n",
    "            \n",
    "            #only update weights when there are similar items                \n",
    "            if len(N_k_i_u)!=0:\n",
    "                N_k_i_u_norm_sqrt=N_k_i_u.shape[0]**0.5                \n",
    "                w_ij[i][N_k_i_u]=w_ij[i][N_k_i_u]+alpha*(\n",
    "                    epsilon_ui*deviation/N_k_i_u_norm_sqrt-lambda_*w_ij[\n",
    "                        i][N_k_i_u])\n",
    "                w_ij[N_k_i_u][:,i]=w_ij[i][N_k_i_u]\n",
    "                                        \n",
    "    return error,b_u,b_i,w_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#koren weighted neighborhood model\n",
    "def koren_neighbor(arr,similarity_matrix,\n",
    "                   miu_init=None,b_u_init=[],\n",
    "                   b_i_init=[],w_ij_init=[],\n",
    "                   alpha=0.005,lambda_=0.02,\n",
    "                   tau=0.0001,top_k=10,\n",
    "                   max_iter=20,diagnosis=True):\n",
    "\n",
    "    #initialize\n",
    "    stop=False\n",
    "    counter=0\n",
    "    sse=None\n",
    "    \n",
    "    #global mean\n",
    "    if not miu_init:       \n",
    "        miu=arr[~np.isnan(arr)].mean()\n",
    "    else:\n",
    "        miu=miu_init\n",
    "        \n",
    "    #user baseline\n",
    "    if len(b_u_init)==0:\n",
    "        b_u=np.zeros(arr.shape[1])\n",
    "    else:\n",
    "        b_u=b_u_init\n",
    "    \n",
    "    #item baseline\n",
    "    if len(b_i_init)==0:\n",
    "        b_i=np.zeros(arr.shape[0])\n",
    "    else:\n",
    "        b_i=b_i_init\n",
    "        \n",
    "    #weighted neighbor\n",
    "    if len(w_ij_init)==0:\n",
    "        w_ij=np.zeros((arr.shape[0],arr.shape[0]))\n",
    "        w_ij.fill(0.001)\n",
    "    else:\n",
    "        w_ij=w_ij_init\n",
    "    \n",
    "    #gradient descent\n",
    "    while not stop:\n",
    "        \n",
    "        error,b_u,b_i,w_ij=koren_neighbor_epoch(\n",
    "                         arr,miu,b_u,b_i,\n",
    "                         similarity_matrix,\n",
    "                         w_ij,alpha,lambda_,top_k)\n",
    "\n",
    "        counter+=1\n",
    "        \n",
    "        #maximum number of epoch\n",
    "        if counter>=max_iter:\n",
    "            stop=True\n",
    "            if diagnosis:\n",
    "                print('Not converged.',\n",
    "                      'Consider increase number of iterations or tolerance')\n",
    "                \n",
    "        #use sum of squared error to determine if converged\n",
    "        sse_prev=sse\n",
    "        sse=error\n",
    "        if sse_prev and abs(sse/sse_prev-1)<=tau:\n",
    "            stop=True\n",
    "            if diagnosis:\n",
    "                print(f'{counter} iterations to reach convergence\\n')\n",
    "\n",
    "    return b_u,b_i,w_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain msd similarity matrix\n",
    "@numba.njit\n",
    "def get_msd_similarity_matrix(arr):\n",
    "\n",
    "    similarity_matrix=np.zeros((arr.shape[1],arr.shape[1]))\n",
    "    for u in range(arr.shape[1]):\n",
    "        for v in range(u+1,arr.shape[1]):\n",
    "            \n",
    "            #self correlation distorts knn selection\n",
    "            if u==v:\n",
    "                continue\n",
    "\n",
    "            #compute msd first then eliminate nan\n",
    "            I_uv=np.square(arr[:,u]-arr[:,v])\n",
    "            valid_I_uv=I_uv[~np.isnan(I_uv)]\n",
    "\n",
    "            #avoid the case where two users havent rated any items in common\n",
    "            if len(valid_I_uv)>0:\n",
    "                msd=1/(valid_I_uv.sum()/len(valid_I_uv)+1)\n",
    "            else:\n",
    "                msd=0\n",
    "\n",
    "            #symmetric matrix\n",
    "            similarity_matrix[u,v]=msd\n",
    "            similarity_matrix[v,u]=msd\n",
    "            \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute item similarity matrix\n",
    "similarity_matrix=get_msd_similarity_matrix(arr_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 iterations to reach convergence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize\n",
    "num_of_latent_factors=40\n",
    "max_num_of_epoch=150\n",
    "learning_rate=0.01\n",
    "lagrange_multiplier=0.02\n",
    "tolerance=0.01\n",
    "top_k=10\n",
    "#koren neighbor\n",
    "b_u,b_i,w_ij=koren_neighbor(arr_train,\n",
    "                   similarity_matrix,\n",
    "                   alpha=learning_rate,\n",
    "                   lambda_=lagrange_multiplier,\n",
    "                   tau=tolerance,top_k=10,\n",
    "                   max_iter=max_num_of_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute global mean\n",
    "miu=arr_train[~np.isnan(arr_train)].mean()\n",
    "\n",
    "#compute deviation\n",
    "deviation=arr_train-np.repeat(\n",
    "            b_u.reshape(1,-1),\n",
    "            arr_train.shape[0],axis=0)+np.repeat(\n",
    "            b_i.reshape(-1,1),\n",
    "    arr_train.shape[1],axis=1)-miu\n",
    "\n",
    "#set nan to zero for dot product\n",
    "deviation[np.isnan(deviation)]=0\n",
    "\n",
    "#find top k neighbors for each item\n",
    "top_k_neighbors=np.argsort(similarity_matrix,axis=0)[-top_k:]\n",
    "\n",
    "#identify the col index of top k neighbors\n",
    "col=top_k_neighbors.flatten()\n",
    "\n",
    "#identify the row index of each item\n",
    "row=np.array([j for i in range(arr_train.shape[0]) for j in [i]*top_k])\n",
    "\n",
    "#create weights matrix\n",
    "weights=np.zeros(w_ij.shape)\n",
    "\n",
    "#only keep weights of top k neighbors\n",
    "weights[(row,col)]=w_ij[(row,col)]\n",
    "\n",
    "#compute influence from weighted neighbors\n",
    "weighted_neighbors=weights@deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix completion\n",
    "output=miu+np.repeat(\n",
    "            b_u.reshape(1,-1),\n",
    "            arr_train.shape[0],axis=0)+np.repeat(\n",
    "            b_i.reshape(-1,1),\n",
    "    arr_train.shape[1],axis=1)+weighted_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top k neighbors for each item,\n",
    "top_k=10\n",
    "top_k_neighbors=np.argsort(similarity_matrix,axis=0)[-top_k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 920, 1633, 2407,  786, 3335, 3654,  484, 4337,  556, 3776],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_neighbors[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1877,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39831661, 0.40135909, 0.45241777, ..., 0.40745266, 0.49127164,\n",
       "       0.47545638])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,100][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1373,  366, 1352, 1246,  959,  750,  256,  525,  388, 1721],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_already_rated_items(user_id, output, df,arr, N=10):\n",
    "    \"\"\"\n",
    "    Filter already rated items and recommend top-N items to the user.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): ID of the user (e.g., 100 for the 100th user).\n",
    "        output (np.ndarray): Predicted ratings for all items (shape: num_items).\n",
    "        df (pd.DataFrame): Initial DataFrame containing user-item ratings.\n",
    "        N (int): Number of top recommendations to generate (default: 10).\n",
    "\n",
    "    Returns:\n",
    "        list: Top-N recommended item indices (excluding already rated items).\n",
    "    \"\"\"\n",
    "    # Get the row index corresponding to the user ID\n",
    "    user_row_index = user_id  # Assuming user IDs match row indices\n",
    "\n",
    "    # Extract the user's actual ratings from the initial DataFrame\n",
    "    user_ratings = df.loc[user_row_index].values\n",
    "\n",
    "    # Filter out items that the user has already rated\n",
    "    unrated_items = np.where(np.isnan(arr[user_id]))[0]\n",
    "    \n",
    "    # Sort unrated items by predicted ratings (highest to lowest)\n",
    "    sorted_indices = np.argsort(output[unrated_items,user_id])[::-1]\n",
    "    \n",
    "    # Get the top-N unrated items (excluding already rated items)\n",
    "    top_N_items = sorted_indices[:N]\n",
    "\n",
    "    return top_N_items\n",
    "\n",
    "# Example usage:\n",
    "# user_id: ID of the 100th user\n",
    "# output: Predicted ratings for all items (shape: num_items)\n",
    "# df: Initial DataFrame containing user-item ratings\n",
    "top_N_recommendations = filter_already_rated_items(204, output, df,arr, N=10)\n",
    "top_N_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 repeats 1448 times\n",
      "366 repeats 1341 times\n",
      "388 repeats 1311 times\n",
      "525 repeats 1167 times\n",
      "1721 repeats 1126 times\n",
      "1372 repeats 1023 times\n",
      "1351 repeats 1014 times\n",
      "1245 repeats 952 times\n",
      "750 repeats 866 times\n",
      "958 repeats 852 times\n"
     ]
    }
   ],
   "source": [
    "# Gather the results\n",
    "big_array = np.concatenate([filter_already_rated_items(i, output, df,arr, N=10) for i in range(output.shape[1])])\n",
    "\n",
    "# Find the top 10 recurring values\n",
    "unique_values, counts = np.unique(big_array, return_counts=True)\n",
    "sorted_indices = np.argsort(counts)[::-1]\n",
    "top_10_values = unique_values[sorted_indices[:10]]\n",
    "top_10_counts = counts[sorted_indices[:10]]\n",
    "\n",
    "# Reveal the secrets\n",
    "for value, count in zip(top_10_values, top_10_counts):\n",
    "    print(f\"{value} repeats {count} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
